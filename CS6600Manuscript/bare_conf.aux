\relax 
\citation{dexter}
\citation{stachniss}
\citation{thrun}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {I-A}}The robot platform}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {I-B}}EKF SLAM}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Problem description}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}The Bayes Filter}{1}}
\newlabel{BayesRule}{{1}{1}}
\newlabel{BayesRuleEta}{{2}{1}}
\newlabel{BayesRuleCondZ}{{3}{1}}
\citation{thrun}
\newlabel{TargetPosterior}{{4}{2}}
\newlabel{SimplTargPost}{{5}{2}}
\newlabel{MoreSimplTargPost}{{6}{2}}
\newlabel{belief}{{7}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}The Bayes Filter Algorithm}{2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Bayes Filter Algorithm}}{2}}
\newlabel{AlgBayes}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}The Kalman Filter}{2}}
\newlabel{1dNormal}{{13}{2}}
\newlabel{MvNormal}{{14}{2}}
\newlabel{Kalman1}{{15}{3}}
\newlabel{Kalman2}{{16}{3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Kalman Filter Algorithm}}{3}}
\newlabel{AlgKalman}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of Kalman filters: (a) initial belief, (b) a measurement (in bold) with the associated uncertainty, (c) belief after integrating the measurement into the belief using the Kalman filter algorithm, (d) belief after motion to the right (which introduces uncertainty), (e) a new measurement with associated uncertainty, and (f) the resulting belief.}}{3}}
\newlabel{KalmanIllustration}{{1}{3}}
\citation{thrun}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Linear transformation of a Gaussian random variable. The lower right plot show the density of the original random variable, X. This random variable is passed through the function displayed in the upper right graph (the transformation of the mean is indicated by the dotted line). The density of the resulting random variable Y is plotted in the upper left graph.}}{4}}
\newlabel{KalmanXform}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-D}}The Extended Kalman Filter}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of linearization applied by the EKF. Instead of passing the Gaussian through the nonlinear function g, it is passed through a linear approximation of g. The linear function is tangent to g at the mean of the original Gaussian. The resulting Gaussian is shown as the dashed line in the upper left graph. The linearization incurs an approximation error, as indicated by the mismatch between the linearized Gaussian (dashed) and the Gaussian computed from the highly accurate Monte-Carlo estimate (solid).}}{4}}
\newlabel{EkfXform}{{3}{4}}
\newlabel{transition}{{17}{4}}
\newlabel{measurement}{{18}{4}}
\bibcite{dexter}{1}
\bibcite{slam1}{2}
\bibcite{slam2}{3}
\bibcite{thrun}{4}
\bibcite{stachniss}{5}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Extended Kalman Filter Algorithm}}{5}}
\newlabel{AlgEkf}{{3}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Approach/Methods}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Future work}{5}}
\@writefile{toc}{\contentsline {section}{References}{5}}
